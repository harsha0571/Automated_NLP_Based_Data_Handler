{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import fastparquet as fp\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated.json', 'r') as f:\n",
    "    data_json = json.load(f)\n",
    "    \n",
    "len_json = len(data_json)\n",
    "\n",
    "print(\"length of json: \",len_json)\n",
    "\n",
    "if \"doc_info.parquet\" not in os.listdir(\".\"):\n",
    "    len_doc = 0\n",
    "else:\n",
    "    len_doc = pq.read_metadata(\"doc_info.parquet\").num_rows\n",
    "    \n",
    "print(\"length of docinfo: \",len_doc)\n",
    "\n",
    "try:\n",
    "    for i,keyword_obj in enumerate(data_json):\n",
    "        \n",
    "        key_loc = keyword_obj['doc_loc']\n",
    "        key_arr = keyword_obj['keywords_array']\n",
    "\n",
    "        for keyarr_obj in range(len(key_arr)):\n",
    "            \n",
    "            key_obj = key_arr[keyarr_obj]\n",
    "            keyword_name = key_obj['keyword']\n",
    "            keyword_score = key_obj['score']\n",
    "            print(i,keyword_name,keyword_score)\n",
    "\n",
    "            if keyword_name + \"Index.parquet\" not in os.listdir(\".\"):\n",
    "                \n",
    "                dict = {\"id\" : len_doc, \"score\":keyword_score}\n",
    "                df = pl.from_dict(data = dict, schema={\"id\" : pl.Int32, \"score\" : pl.Float64})\n",
    "                df = df.to_pandas()\n",
    "                df.to_parquet(\"./\" + keyword_name + \"Index.parquet\")\n",
    "\n",
    "        if \"doc_info.parquet\" not in os.listdir(\".\"):\n",
    "            dict = {\"id\" : [],\"location\":[]}\n",
    "            df = pl.from_dict(data = dict, schema={\"id\" : pl.Int32, \"location\" : pl.Utf8})\n",
    "            df = df.to_pandas()\n",
    "            df.to_parquet(\"./doc_info.parquet\")\n",
    "            \n",
    "        data = {\"id\" : [len_doc + i],\"location\": [key_loc] }\n",
    "        df1 = pd.DataFrame(data)\n",
    "        fp.write(\"doc_info.parquet\",df1,append = True)\n",
    "        \n",
    "except:\n",
    "    print(\"Some error occured\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapping docid docloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def append_data(json_data,dataset):\n",
    "\n",
    "    len_dset = len(dataset)\n",
    "\n",
    "    for i,val in enumerate(json_data):\n",
    "        data = np.array([(len_dset + i , val['doc_loc'])], dtype=[('doc_id', 'int32'), ('doc_loc', 'S10')])\n",
    "        dataset.resize(dataset.shape[0]+1, axis=0)   \n",
    "        dataset[len_dset + i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_json_data 2\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open('generated.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "len_json_data = len(json_data)\n",
    "print(\"len_json_data\",len_json_data)\n",
    "\n",
    "file = h5py.File('doc_info1.hdf5', 'a')\n",
    "\n",
    "if 'my_dataset' not in file:\n",
    "    dataset = file.create_dataset('my_dataset', shape=(0,), dtype=[('doc_id', 'int32'), ('doc_loc', 'S10')],maxshape=(None,))\n",
    "else:\n",
    "    dataset = file['my_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len dset:  2\n",
      "0 (0, b'640c08980f')\n",
      "1 (1, b'640c0898a3')\n"
     ]
    }
   ],
   "source": [
    "append_data(json_data,dataset)\n",
    "\n",
    "print(\"len dset: \",len(dataset))\n",
    "\n",
    "for i,val in enumerate(dataset):\n",
    "    print(i,val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generating keywords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, b'640c08980f') (1, b'640c0898a3')]\n",
      "{0: '640c08980f', 1: '640c0898a3'}\n"
     ]
    }
   ],
   "source": [
    "import vaex\n",
    "import json \n",
    "import h5py\n",
    "\n",
    "# df = vaex.open('doc_info1.hdf5')\n",
    "df = h5py.File('doc_info1.hdf5', 'a')\n",
    "# for i,val in enumerate(df.items()):\n",
    "#     print(i,val)\n",
    "\n",
    "dataset = df['my_dataset']\n",
    "data = dataset[:]\n",
    "\n",
    "print((data))\n",
    "# print((data)[0])\n",
    "doc_map = {}\n",
    "# print(len(data))\n",
    "for i in range(len(data)):\n",
    "    # print(data)\n",
    "    doc_id = data[i][0]\n",
    "    doc_location = data[i][1].decode('UTF-8')\n",
    "    \n",
    "    doc_map[doc_id] = doc_location\n",
    "\n",
    "# for doc_id, doc_location in doc_map.items():\n",
    "#     print(doc_id,doc_location)\n",
    "print(doc_map)\n",
    "with open('generated.json', 'r') as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "# # print(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword_obj in data_json:\n",
    "    key_arr = keyword_obj['keywords_array']\n",
    "    \n",
    "    for keyarr_obj in range(len(key_arr)):\n",
    "\n",
    "        key_obj = key_arr[keyarr_obj]\n",
    "        # print(key_obj)\n",
    "        keyword_name = key_obj['keyword']\n",
    "        keyword_score = key_obj['score']\n",
    "        \n",
    "        # print(keyword_name,keyword_score)\n",
    "\n",
    "        file2 = h5py.File(keyword_name + \"Index.hdf5\", 'a')\n",
    "        file3 = h5py.File(keyword_name + \"Bias.hdf5\", 'a')\n",
    "        \n",
    "        if keyword_name not in file2:\n",
    "            dataset2 = file2.create_dataset(keyword_name, shape=(0,), maxshape=(None,), dtype = [('id', int), ('score', float)])\n",
    "        else:\n",
    "            dataset2 = file2[keyword_name]\n",
    "        \n",
    "        if keyword_name not in file3:\n",
    "            dataset3 = file3.create_dataset(keyword_name, shape=(0,), maxshape=(None,), dtype = [('id', int),('bias', int)])\n",
    "        else:\n",
    "            dataset3 = file3[keyword_name]\n",
    "\n",
    "        for doc_id, doc_location in doc_map.items():\n",
    "\n",
    "            if doc_location in keyword_obj['doc_loc']:\n",
    "                \n",
    "                dataset2.resize((dataset2.shape[0] + 1,))\n",
    "                dataset3.resize((dataset3.shape[0] + 1,))\n",
    "                # print(doc_location,keyword_score)\n",
    "                # print(type(int(doc_location)),type(keyword_score))\n",
    "                dataset2[-1] = (doc_id, keyword_score)\n",
    "                dataset3[-1:] = (doc_id, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
