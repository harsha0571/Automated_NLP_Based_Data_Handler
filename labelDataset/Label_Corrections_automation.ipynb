{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"final_labeled.txt\",\"r\")\n",
    "lines = file.readlines()\n",
    "count = 1\n",
    "error=[]\n",
    "ss=[]\n",
    "t=[]\n",
    "kw=[]\n",
    "for l in lines:\n",
    "    title,keywords = l.split(\"\\t\")\n",
    "    t.append(title)\n",
    "    kw.append(keywords)\n",
    "    keywords = keywords.replace(\"\\n\",\"\")\n",
    "    keywords = keywords.split(\"_\")\n",
    "    words = title.split(\" \")\n",
    "    # print(\"Title: \",words)\n",
    "    # print(\"Keywords: \",keywords)\n",
    "    for k in keywords:\n",
    "        if k not in words:\n",
    "            error.append(count)\n",
    "            break\n",
    "    for w in words:\n",
    "        if w[-1]=='s':\n",
    "            ss.append(count)\n",
    "            break\n",
    "    count+=1\n",
    "print(error)\n",
    "# print(ss)\n",
    "# print(t[123])\n",
    "# print(kw[123].replace(\"\\n\",\"\").split(\"_\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Harsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')  # downloads the large nltk corpus of words\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # loads spacy's english core library\n",
    "sw_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "file = open(\"label.txt\",\"r\")\n",
    "lines = file.readlines()\n",
    "t=[]\n",
    "k=[]\n",
    "\n",
    "\n",
    "# remove stopwords from text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    res = []\n",
    "    for word in text:\n",
    "        if word not in sw_spacy:\n",
    "            res.append(word)\n",
    "    return res\n",
    "\n",
    "for l in lines:\n",
    "    title,keywords = l.split(\"\\t\")\n",
    "    \n",
    "    keywords = keywords.replace(\"\\n\",\"\")\n",
    "    keywords = keywords.split(\"_\")\n",
    "    words = title.split(\" \")\n",
    "    kl = []\n",
    "    for key in keywords:\n",
    "        if key not in words:\n",
    "            continue\n",
    "        if key not in kl:\n",
    "            kl.append(key)\n",
    "    t.append(title)\n",
    "    k.append(kl)\n",
    "    \n",
    "for i in range(len(t)):\n",
    "    f = open('final_labeled.txt', 'a')\n",
    "    test = remove_stopwords(k[i])\n",
    "    if len(test) == 0:\n",
    "        f.write(t[i] + \"\\t\" + '_'.join(key for key in k[i]) + \"\\n\")\n",
    "    else:\n",
    "        f.write(t[i] + \"\\t\" + '_'.join(key for key in remove_stopwords(k[i])) + \"\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Harsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')  # downloads the large nltk corpus of words\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # loads spacy's english core library\n",
    "sw_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "# remove stopwords from text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    res = []\n",
    "    for word in text:\n",
    "        if word not in sw_spacy:\n",
    "            res.append(word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = remove_stopwords(k[1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "daca86dfe90386ebecafa86302dd102978461e1263ffd5af8c9f541b523ba9f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
